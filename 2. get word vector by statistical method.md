In previous section in which we go through the whole RAG process, we shown that we need to chop documents into smaller pieces and convert them into some kind of math vector. In this section, let's see how we can convert document pieces into
vectors and we can search given document section as question context to improve the response quality of llm.

The first concept we need to know is TF-IDF which is “Term Frequency-Inverse Document Frequency”, is a kind of statistical measure to evaluate the importance of word in a document relative to a collection of documents. It needs to compute two
terms, The first is called term frequency which is computed as following:

TF(t,d) = (number of times of word t happends in d) / (total count of words in document d) 

The second term we need to compute is IDF, which is "inverse document frequency", which is as following:

IDF(t) = log[( total number of documents) / (number of doucuments containing word t)]

if a word happends in many documents, such word will not have important value such as "is", "the", "a", "an".

We combine the two terms together to get TF-IDF:

TF-IDF(t, d) = TF(t,d) * IDF(t)

For above formula, the importance of a word is decided by two factors, the first factor is times it appears in given document, the more appearances in the document, the more important for the word, but we need to mitigate common words like
"a", "an", "the", such words are easy to appear in given document but without given any importance info for the document, and such word would appear in many documents. Therefore if a given word can easily appear in many documents, then its 
importance should be lower. We multiply these two factors together to combine the effectiveness of them.

Let's use some code to implement the TF-IDF algorithm as following:

```py
import math
from collections import Counter 

def compute_tf(doc):
  """
  compute each frequency of each word in the document
  """
  word_count = Counter(doc)
  total_terms = len(doc)
  return {term: count / total_terms for term ,count in word_count.items()}

def compute_idf(all_docs):
  """
  IDF(t) = log[( total number of documents) / (number of doucuments containing word t)]
  """
  total_docs = len(all_docs)
  idf = {}
  for doc in all_docs:
    for term in set(doc):
      #compute number of doc containing the given term
      idf[term] = idf.get(term, 0) + 1
  return {term: math.log(total_docs / count) for term, count in idf.items()}

def compute_tf_idf(all_docs):
  """
  TF-IDF(t, d) = TF(t,d) * IDF(t)
  """
  tf_list = [compute_tf(doc) for doc in all_docs]
  idf = compute_idf(all_docs)
  tf_idf_list = []
  for tf in tf_list:
    tf_idf = {term: tf[term] * idf.get(term, 0) for term in tf}
    tf_idf_list.append(tf_idf)
  
  return tf_idf_list 


all_docs = [
    ["this", "is", "a", "sample"],
    ["this", "is", "another", "example", "example"]
]

td_idf_scores = compute_tf_idf(all_docs)

for i, doc_scores in enumerate(td_idf_scores):
  print(f"document: {i+1} TF-IDF scores:")
  for term, score in doc_scores.items():
    print(f" {term}: {score:.4f}")
    
```

Running above code we get the following result:

```py
document: 1 TF-IDF scores:
 this: 0.0000
 is: 0.0000
 a: 0.1733
 sample: 0.1733
document: 2 TF-IDF scores:
 this: 0.0000
 is: 0.0000
 another: 0.1386
 example: 0.2773
```

Base on the TF-IDF score for each word in document, let's see how to convert a word into vector, given documents as following:

```py
all_docs = [
    ["this", "is", "a", "sample", "document"],
    ["this", "document", "is", "another", "example"],
    ["one", "more", "sample", "document", "example"]
]
```

1, compute tf-idf value for each word, remove any repeation of word in all_docs and sort them:

```py
#1. remove repeation of word in documents and sort all words from document then each word will have its order, and compute tf-idf value of each word
idf = compute_idf(all_docs)
print(f"idf : {idf}")

vocab = sorted(idf.keys())
print(f"vocab: {vocab}")
```
Running above code we get the following result:

```py
idf : {'a': 1.0986122886681098, 'document': 0.0, 'this': 0.4054651081081644, 'sample': 0.4054651081081644, 'is': 0.4054651081081644, 'example': 0.4054651081081644, 'another': 1.0986122886681098, 'more': 1.0986122886681098, 'one': 1.0986122886681098}
vocab: ['a', 'another', 'document', 'example', 'is', 'more', 'one', 'sample', 'this']
```
Each word will have an order value such as 'a' has value 0, 'another' has value 1...

2, convert a document into a vector, that is take each word from the vocab, if the word appear in the doc, 
replace the word with its tf-idf value at the given index of the vector, the index is the order value of the word in the vocab, if the given word is not in the document,set the value at the given index to 0
```py
def compute_tf_idf_vector(doc, idf):
  tf = compute_tf(doc)
  vec =  {term: tf[term] * idf.get(term, 0) for term in doc}
  print(f"doc: {doc} and word to tf-idf value: {vec}")
  return vec

def doc2vec(doc, idf):
  vec = compute_tf_idf_vector(doc, idf)
  vector = [vec.get(term, 0) for term in vocab]
  return vector

vectors = []
for i,doc in enumerate(all_docs):
  vec = doc2vec(doc, idf)
  print(f"vector of document {i} is : {vec}")
  vectors.append(vec)
```
Running above code we get following result:

```py
the 0th doc is : ['this', 'is', 'a', 'sample', 'document']
doc: ['this', 'is', 'a', 'sample', 'document'] and word to tf-idf value: {'this': 0.08109302162163289, 'is': 0.08109302162163289, 'a': 0.21972245773362198, 'sample': 0.08109302162163289, 'document': 0.0}
vector of document 0 is : [0.21972245773362198, 0, 0.0, 0, 0.08109302162163289, 0, 0, 0.08109302162163289, 0.08109302162163289]
the 1th doc is : ['this', 'document', 'is', 'another', 'example']
doc: ['this', 'document', 'is', 'another', 'example'] and word to tf-idf value: {'this': 0.08109302162163289, 'document': 0.0, 'is': 0.08109302162163289, 'another': 0.21972245773362198, 'example': 0.08109302162163289}
vector of document 1 is : [0, 0.21972245773362198, 0.0, 0.08109302162163289, 0.08109302162163289, 0, 0, 0, 0.08109302162163289]
the 2th doc is : ['one', 'more', 'sample', 'document', 'example']
doc: ['one', 'more', 'sample', 'document', 'example'] and word to tf-idf value: {'one': 0.21972245773362198, 'more': 0.21972245773362198, 'sample': 0.08109302162163289, 'document': 0.0, 'example': 0.08109302162163289}
vector of document 2 is : [0, 0, 0.0, 0.08109302162163289, 0, 0.21972245773362198, 0.21972245773362198, 0.08109302162163289, 0]
```

3. for given word, we find its index number from vocab, and go to each document vector, get the value from the given index and combine all the value as a vector for the given word:

```py
def get_word_vec(word):
  word_index = vocab.index(word)
  print(f"word index: {word_index}")
  word_vec = []
  for vector in vectors:
      val = vector[word_index]
      word_vec.append(val)

  return word_vec

word = "sample"
vec = get_word_vec(word)
print(f"vector for word: {word} is {vec}")
```

Running above code we get following result:

```py
word index: 7
vector for word: sample is [0.08109302162163289, 0, 0.08109302162163289]
```

Now we can compute consin similarity for given vectors as following:
Cosin Similarity = (A . B) / ||A|| * ||B||

the dot operation of two vector of (x1, y1) , (x2 , y2) is x1*x2 + y1 * y2, the ||A|| is norm value, if A is (x, y), then its norm is sqrt(x**2 + y**2) , let's check the code as following:

```py
#compute cosin similarity
import math

def dot_product(v1, v2):
  return sum(x * y for x,y in zip(v1,v2))

def norm(vec):
  return math.sqrt(sum(x ** 2 for x in vec))

def consin_similarity(v1, v2):
  dot = dot_product(v1, v2)
  norm1 = norm(v1)
  norm2 = norm(v2)
  if norm1 == 0 or norm2 == 0:
    return 0.0
  return dot / (norm1 * norm2)

word1 = "sample"
word2 = "example"

v1 = get_word_vec(word1)
v2 = get_word_vec(word2)

print(f"v1: {v1}")
print(f"v2: {v2}")

sim = consin_similarity(v1, v2)
print(f"consin similarity is: {sim}")
```

Runnint above code we can get following result:

```py
word index: 7
word index: 3
v1: [0.08109302162163289, 0, 0.08109302162163289]
v2: [0, 0.08109302162163289, 0.08109302162163289]
consin similarity is: 0.5000000000000001
```

Actually we don'd need to do all those troubles by our own, we can directly use the same algorithm by calling sklearn lib as following:

```py
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np 

all_docs = [
     "this is a sample document",
    "this document is another example",
    "one more sample document example"
]

vectorizer = TfidfVectorizer()
#convert each document to vector
doc_vecs = vectorizer.fit_transform(all_docs)
"""
(0, 7)  0.5268 means for the matrix, the value of cell with row 0 and column 7
is 0.5268, if not cell metioned , then the value of cell is 0
"""
print(f"document vecs max with compression: {doc_vecs}")
doc_dense_vecs = doc_vecs.todense()
print(f"dense vecs: {doc_dense_vecs}")

vocab = vectorizer.get_feature_names_out()
print(f"vocab: {vocab}")

for i, doc_vec in enumerate(doc_dense_vecs):
  print(f"doc {i+1} with TF-IDF scores: {doc_vec}")

word = "example"
word_index = vectorizer.vocabulary_.get(word)
print(f"index for word:{word} is :{word_index}")
if word_index is not None:
  print(f"\n vector for word:{word}:")
  print(doc_dense_vecs[:,word_index])

from sklearn.metrics.pairwise import cosine_similarity
word_example_vec = np.asarray(doc_dense_vecs[:,word_index]).T
print(f"vec for example after flattern: {word_example_vec}")
word = "sample"
word_index = vectorizer.vocabulary_.get(word)
word_sample_vec = np.asarray(doc_dense_vecs[:,word_index]).T
print(f"vec for sample after flattern: {word_sample_vec}")
similarity = cosine_similarity(word_example_vec, word_sample_vec)
print(f"similarity is: {similarity}")
```

In above code, the vectorizer will do what we have done before, it will compute the TF-IDF of each word, then convert document into vocab as before, and compute the TF-IDF vector for each document, all this wil be done by the call of 
vectorizer.fit_transform(all_docs). And the vocab we get before can be done by the call of vectorizer.get_feature_names_out().

And the doc_dense_vecs is the result of convert each document into a vector, and we can get each word vector by getting the index of the word from vocab and the getting each value for the given index from each document vector, and finnaly
we can compute the cosine similarity by using cosine_similarity from sklearn.metrics.pairwise, running above code, we get following result:

```py
document vecs max with compression:   (0, 7)	0.5268201732399633
  (0, 3)	0.5268201732399633
  (0, 6)	0.5268201732399633
  (0, 1)	0.40912286076708654
  (1, 7)	0.43306684852870914
  (1, 3)	0.43306684852870914
  (1, 1)	0.33631504064053513
  (1, 0)	0.5694308628404254
  (1, 2)	0.43306684852870914
  (2, 6)	0.4061917781433946
  (2, 1)	0.3154441510317797
  (2, 2)	0.4061917781433946
  (2, 5)	0.5340933749435833
  (2, 4)	0.5340933749435833
dense vecs: [[0.         0.40912286 0.         0.52682017 0.         0.
  0.52682017 0.52682017]
 [0.56943086 0.33631504 0.43306685 0.43306685 0.         0.
  0.         0.43306685]
 [0.         0.31544415 0.40619178 0.         0.53409337 0.53409337
  0.40619178 0.        ]]
vocab: ['another' 'document' 'example' 'is' 'more' 'one' 'sample' 'this']
doc 1 with TF-IDF scores: [[0.         0.40912286 0.         0.52682017 0.         0.
  0.52682017 0.52682017]]
doc 2 with TF-IDF scores: [[0.56943086 0.33631504 0.43306685 0.43306685 0.         0.
  0.         0.43306685]]
doc 3 with TF-IDF scores: [[0.         0.31544415 0.40619178 0.         0.53409337 0.53409337
  0.40619178 0.        ]]
index for word:example is :2

 vector for word:example:
[[0.        ]
 [0.43306685]
 [0.40619178]]
vec for example after flattern: [[0.         0.43306685 0.40619178]]
vec for sample after flattern: [[0.52682017 0.         0.40619178]]
similarity is: [[0.41772158]]
```

Now we can do our RAG base on cosine similarity as following:

```py
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def compute_cosine_similarity(text1, text2):
  vectorizer = TfidfVectorizer()
  doc_vecs = vectorizer.fit_transform([text1, text2])
  doc_dense_vecs = doc_vecs.todense()
  vec1 = np.asarray(doc_dense_vecs[0])
  vec2 = np.asarray(doc_dense_vecs[1])
  return cosine_similarity(vec1, vec2)
  

def find_best_match_piece(query, db_records):
  best_score = 0
  best_record = 0
  for record in db_records:
    #current_score = similarity_by_matching_common_words(query, record)
    current_score = compute_cosine_similarity(query, record)
    if current_score > best_score:
      best_score = current_score
      best_record = record

  return best_score, best_record


best_score, best_record = find_best_match_piece(query, db_records)

print(f"best score:{best_score}, best record: {best_record}")
```
Running above code we get the following output:

```py
best score:[[0.13627634]], best record: A RAG vector store is a database or dataset that contains vectorized data points.
```
coincidently we still get the same output as last section, therefore the response from llm will still be the same. Converting text into vector will have its advantage when the dataset is huge,
such as you have millions of complex documents.Doing vector cosine computation is much easier and quicker than getting common word counts.


