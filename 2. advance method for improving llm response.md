In previous section in which we go through the whole RAG process, we shown that we need to chop documents into smaller pieces and convert them into some kind of math vector. In this section, let's see how we can convert document pieces into
vectors and we can search given document section as question context to improve the response quality of llm.

The first concept we need to know is TF-IDF which is “Term Frequency-Inverse Document Frequency”, is a kind of statistical measure to evaluate the importance of word in a document relative to a collection of documents. It needs to compute two
terms, The first is called term frequency which is computed as following:

TF(t,d) = (number of times of word t happends in d) / (total count of words in document d) 

The second term we need to compute is IDF, which is "inverse document frequency", which is as following:

IDF(t) = log[( total number of documents) / (number of doucuments containing word t)]

if a word happends in many documents, such word will not have important value such as "is", "the", "a", "an".

We combine the two terms together to get TF-IDF:

TF-IDF(t, d) = TF(t,d) * IDF(t)

For above formula, the importance of a word is decided by two factors, the first factor is times it appears in given document, the more appearances in the document, the more important for the word, but we need to mitigate common words like
"a", "an", "the", such words are easy to appear in given document but without given any importance info for the document, and such word would appear in many documents. Therefore if a given word can easily appear in many documents, then its 
importance should be lower. We multiply these two factors together to combine the effectiveness of them.

Let's use some code to implement the TF-IDF algorithm as following:

```py
import math
from collections import Counter 

def compute_tf(doc):
  """
  compute each frequency of each word in the document
  """
  word_count = Counter(doc)
  total_terms = len(doc)
  return {term: count / total_terms for term ,count in word_count.items()}

def compute_idf(all_docs):
  """
  IDF(t) = log[( total number of documents) / (number of doucuments containing word t)]
  """
  total_docs = len(all_docs)
  idf = {}
  for doc in all_docs:
    for term in set(doc):
      #compute number of doc containing the given term
      idf[term] = idf.get(term, 0) + 1
  return {term: math.log(total_docs / count) for term, count in idf.items()}

def compute_tf_idf(all_docs):
  """
  TF-IDF(t, d) = TF(t,d) * IDF(t)
  """
  tf_list = [compute_tf(doc) for doc in all_docs]
  idf = compute_idf(all_docs)
  tf_idf_list = []
  for tf in tf_list:
    tf_idf = {term: tf[term] * idf.get(term, 0) for term in tf}
    tf_idf_list.append(tf_idf)
  
  return tf_idf_list 


all_docs = [
    ["this", "is", "a", "sample"],
    ["this", "is", "another", "example", "example"]
]

td_idf_scores = compute_tf_idf(all_docs)

for i, doc_scores in enumerate(td_idf_scores):
  print(f"document: {i+1} TF-IDF scores:")
  for term, score in doc_scores.items():
    print(f" {term}: {score:.4f}")
    
```

Running above code we get the following result:

```py
document: 1 TF-IDF scores:
 this: 0.0000
 is: 0.0000
 a: 0.1733
 sample: 0.1733
document: 2 TF-IDF scores:
 this: 0.0000
 is: 0.0000
 another: 0.1386
 example: 0.2773
```

Base on the TF-IDF score for each word in document, let's see how to convert a word into vector, given documents as following:

```py
all_docs = [
    ["this", "is", "a", "sample", "document"],
    ["this", "document", "is", "another", "example"],
    ["one", "more", "sample", "document", "example"]
]
```

1, compute tf-idf value for each word, remove any repeation of word in all_docs and sort them:

```py
#1. remove repeation of word in documents and sort all words from document then each word will have its order, and compute tf-idf value of each word
idf = compute_idf(all_docs)
print(f"idf : {idf}")

vocab = sorted(idf.keys())
print(f"vocab: {vocab}")
```
Running above code we get the following result:

```py
idf : {'a': 1.0986122886681098, 'document': 0.0, 'this': 0.4054651081081644, 'sample': 0.4054651081081644, 'is': 0.4054651081081644, 'example': 0.4054651081081644, 'another': 1.0986122886681098, 'more': 1.0986122886681098, 'one': 1.0986122886681098}
vocab: ['a', 'another', 'document', 'example', 'is', 'more', 'one', 'sample', 'this']
```
Each word will have an order value such as 'a' has value 0, 'another' has value 1...

2, convert a document into a vector, that is take each word from the vocab, if the word appear in the doc, 
replace the word with its tf-idf value at the given index of the vector, the index is the order value of the word in the vocab, if the given word is not in the document,set the value at the given index to 0
```py
def compute_tf_idf_vector(doc, idf):
  tf = compute_tf(doc)
  vec =  {term: tf[term] * idf.get(term, 0) for term in doc}
  print(f"doc: {doc} and word to tf-idf value: {vec}")
  return vec

def doc2vec(doc, idf):
  vec = compute_tf_idf_vector(doc, idf)
  vector = [vec.get(term, 0) for term in vocab]
  return vector

vectors = []
for i,doc in enumerate(all_docs):
  vec = doc2vec(doc, idf)
  print(f"vector of document {i} is : {vec}")
  vectors.append(vec)
```
Running above code we get following result:

```py
the 0th doc is : ['this', 'is', 'a', 'sample', 'document']
doc: ['this', 'is', 'a', 'sample', 'document'] and word to tf-idf value: {'this': 0.08109302162163289, 'is': 0.08109302162163289, 'a': 0.21972245773362198, 'sample': 0.08109302162163289, 'document': 0.0}
vector of document 0 is : [0.21972245773362198, 0, 0.0, 0, 0.08109302162163289, 0, 0, 0.08109302162163289, 0.08109302162163289]
the 1th doc is : ['this', 'document', 'is', 'another', 'example']
doc: ['this', 'document', 'is', 'another', 'example'] and word to tf-idf value: {'this': 0.08109302162163289, 'document': 0.0, 'is': 0.08109302162163289, 'another': 0.21972245773362198, 'example': 0.08109302162163289}
vector of document 1 is : [0, 0.21972245773362198, 0.0, 0.08109302162163289, 0.08109302162163289, 0, 0, 0, 0.08109302162163289]
the 2th doc is : ['one', 'more', 'sample', 'document', 'example']
doc: ['one', 'more', 'sample', 'document', 'example'] and word to tf-idf value: {'one': 0.21972245773362198, 'more': 0.21972245773362198, 'sample': 0.08109302162163289, 'document': 0.0, 'example': 0.08109302162163289}
vector of document 2 is : [0, 0, 0.0, 0.08109302162163289, 0, 0.21972245773362198, 0.21972245773362198, 0.08109302162163289, 0]
```

3. for given word, we find its index number from vocab, and go to each document vector, get the value from the given index and combine all the value as a vector for the given word:

```py
def get_word_vec(word):
  word_index = vocab.index(word)
  print(f"word index: {word_index}")
  word_vec = []
  for vector in vectors:
      val = vector[word_index]
      word_vec.append(val)

  return word_vec

word = "sample"
vec = get_word_vec(word)
print(f"vector for word: {word} is {vec}")
```

Running above code we get following result:

```py
word index: 7
vector for word: sample is [0.08109302162163289, 0, 0.08109302162163289]
```
